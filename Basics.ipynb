{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNTKIJsUz73VtABfFVg6lM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neermita18/Deep-Learning/blob/main/Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation with user functions\n"
      ],
      "metadata": {
        "id": "ls2wImbWDsZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gcb9vkvv_Ugu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X= torch.rand(2,3)\n",
        "Y= torch.rand(1,3)\n",
        "W= torch.zeros(2,1)"
      ],
      "metadata": {
        "id": "lYLkc9IUCEyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "  return torch.from_numpy(np.dot(W.T,x))\n",
        "def loss(y,y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "def gradient(x,y,y_pred):\n",
        "  return np.dot(y_pred-y,2*x.T).mean()"
      ],
      "metadata": {
        "id": "nQHAShdYDrmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Basic Backprop and Loss"
      ],
      "metadata": {
        "id": "7GX_utuAKWHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.01\n",
        "i=10\n",
        "for epoch in range(i):\n",
        "  y_pred=forward(X)\n",
        "  l=loss(Y,y_pred)\n",
        "  dw=gradient(X,Y,y_pred)\n",
        "  W-=lr*dw\n",
        "  print(f\"Loss: {l}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-Q4Kk_pEomh",
        "outputId": "bedd037c-f08c-4aee-b7c0-1f2e2818b7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4210244417190552\n",
            "\n",
            "Loss: 0.39814022183418274\n",
            "\n",
            "Loss: 0.37653255462646484\n",
            "\n",
            "Loss: 0.35613036155700684\n",
            "\n",
            "Loss: 0.3368663489818573\n",
            "\n",
            "Loss: 0.3186769187450409\n",
            "\n",
            "Loss: 0.3015022575855255\n",
            "\n",
            "Loss: 0.28528568148612976\n",
            "\n",
            "Loss: 0.2699737250804901\n",
            "\n",
            "Loss: 0.25551605224609375\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VwP4rV1AKZfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backprop with Autograd"
      ],
      "metadata": {
        "id": "iI0sPn5TTK2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=torch.rand(2,3, dtype=torch.float32)\n",
        "Y=torch.rand(1,3,dtype=torch.float32)\n",
        "W=torch.zeros(2,1,dtype=torch.float32,requires_grad=True)"
      ],
      "metadata": {
        "id": "VXDQFZzaE4Uu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "  return torch.matmul(W.T,x)\n",
        "def loss(y,y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "def gradient(x,y,y_pred):\n",
        "  return np.dot(y_pred-y,2*x.T).mean()"
      ],
      "metadata": {
        "id": "mL77oCgUTY9c"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=500\n",
        "lr=0.01\n",
        "for epoch in range(epochs):\n",
        "  y_pred=forward(X)\n",
        "  l=loss(Y,y_pred)\n",
        "  l.backward()\n",
        "  with torch.no_grad():\n",
        "    W-=lr*W.grad #manual updation of weights\n",
        "  W.grad.zero_()\n",
        "  if epoch%10==0:\n",
        "    print(f\"Loss: {l}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cKd8fKrTZl9",
        "outputId": "81b8d275-8053-4176-dccd-f3a1a55af605"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4814772307872772\n",
            "Loss: 0.4138525426387787\n",
            "Loss: 0.3600698709487915\n",
            "Loss: 0.3172636330127716\n",
            "Loss: 0.2831619679927826\n",
            "Loss: 0.25596341490745544\n",
            "Loss: 0.234239861369133\n",
            "Loss: 0.21685890853405\n",
            "Loss: 0.20292268693447113\n",
            "Loss: 0.19171936810016632\n",
            "Loss: 0.182684525847435\n",
            "Loss: 0.17537063360214233\n",
            "Loss: 0.16942287981510162\n",
            "Loss: 0.16455994546413422\n",
            "Loss: 0.16055871546268463\n",
            "Loss: 0.1572422832250595\n",
            "Loss: 0.15447042882442474\n",
            "Loss: 0.15213187038898468\n",
            "Loss: 0.15013845264911652\n",
            "Loss: 0.14842019975185394\n",
            "Loss: 0.14692164957523346\n",
            "Loss: 0.14559878408908844\n",
            "Loss: 0.1444167047739029\n",
            "Loss: 0.14334774017333984\n",
            "Loss: 0.14236991107463837\n",
            "Loss: 0.14146576821804047\n",
            "Loss: 0.14062149822711945\n",
            "Loss: 0.13982607424259186\n",
            "Loss: 0.13907082378864288\n",
            "Loss: 0.1383487433195114\n",
            "Loss: 0.1376543939113617\n",
            "Loss: 0.13698333501815796\n",
            "Loss: 0.13633209466934204\n",
            "Loss: 0.1356978565454483\n",
            "Loss: 0.13507845997810364\n",
            "Loss: 0.13447205722332\n",
            "Loss: 0.13387729227542877\n",
            "Loss: 0.13329298794269562\n",
            "Loss: 0.1327182501554489\n",
            "Loss: 0.1321522742509842\n",
            "Loss: 0.13159452378749847\n",
            "Loss: 0.13104447722434998\n",
            "Loss: 0.13050173223018646\n",
            "Loss: 0.12996597588062286\n",
            "Loss: 0.1294368952512741\n",
            "Loss: 0.12891431152820587\n",
            "Loss: 0.12839795649051666\n",
            "Loss: 0.12788774073123932\n",
            "Loss: 0.12738344073295593\n",
            "Loss: 0.12688499689102173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch functions"
      ],
      "metadata": {
        "id": "u17yYdfzdTLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "I90d8HY0VLf9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=torch.rand(2,3, dtype=torch.float32)\n",
        "Y=torch.rand(1,3,dtype=torch.float32)\n",
        "W=torch.zeros(2,1,dtype=torch.float32,requires_grad=True)"
      ],
      "metadata": {
        "id": "BFUF1S44dWq5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "  return torch.matmul(W.T,x)\n"
      ],
      "metadata": {
        "id": "Vdghn51PdeRe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss= nn.MSELoss()\n",
        "optimizer= torch.optim.SGD([W],lr=0.01)"
      ],
      "metadata": {
        "id": "aY1QK2c1dofB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=100\n",
        "for epoch in range(epochs):\n",
        "  y_pred=forward(X)\n",
        "  l=loss(Y,y_pred)\n",
        "  l.backward()\n",
        "  optimizer.step() #automatic update weights\n",
        "  optimizer.zero_grad()\n",
        "  print(f\"Epoch: {epoch}, Loss: {l}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWRi28jnd1Ao",
        "outputId": "54270419-64e6-4ce3-8463-43e510033115"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.5854000449180603\n",
            "Epoch: 1, Loss: 0.5713006854057312\n",
            "Epoch: 2, Loss: 0.557545006275177\n",
            "Epoch: 3, Loss: 0.5441247820854187\n",
            "Epoch: 4, Loss: 0.5310315489768982\n",
            "Epoch: 5, Loss: 0.5182576775550842\n",
            "Epoch: 6, Loss: 0.5057951807975769\n",
            "Epoch: 7, Loss: 0.49363645911216736\n",
            "Epoch: 8, Loss: 0.4817741811275482\n",
            "Epoch: 9, Loss: 0.4702010452747345\n",
            "Epoch: 10, Loss: 0.4589100778102875\n",
            "Epoch: 11, Loss: 0.4478943347930908\n",
            "Epoch: 12, Loss: 0.4371471405029297\n",
            "Epoch: 13, Loss: 0.426661878824234\n",
            "Epoch: 14, Loss: 0.41643229126930237\n",
            "Epoch: 15, Loss: 0.4064520299434662\n",
            "Epoch: 16, Loss: 0.39671504497528076\n",
            "Epoch: 17, Loss: 0.38721537590026855\n",
            "Epoch: 18, Loss: 0.3779473304748535\n",
            "Epoch: 19, Loss: 0.3689051568508148\n",
            "Epoch: 20, Loss: 0.3600834310054779\n",
            "Epoch: 21, Loss: 0.35147666931152344\n",
            "Epoch: 22, Loss: 0.34307971596717834\n",
            "Epoch: 23, Loss: 0.33488747477531433\n",
            "Epoch: 24, Loss: 0.3268948495388031\n",
            "Epoch: 25, Loss: 0.3190970718860626\n",
            "Epoch: 26, Loss: 0.3114893436431885\n",
            "Epoch: 27, Loss: 0.30406704545021057\n",
            "Epoch: 28, Loss: 0.2968256175518036\n",
            "Epoch: 29, Loss: 0.28976067900657654\n",
            "Epoch: 30, Loss: 0.2828679382801056\n",
            "Epoch: 31, Loss: 0.27614322304725647\n",
            "Epoch: 32, Loss: 0.2695823907852173\n",
            "Epoch: 33, Loss: 0.2631813585758209\n",
            "Epoch: 34, Loss: 0.2569364011287689\n",
            "Epoch: 35, Loss: 0.2508436143398285\n",
            "Epoch: 36, Loss: 0.24489931762218475\n",
            "Epoch: 37, Loss: 0.23909986019134521\n",
            "Epoch: 38, Loss: 0.23344172537326813\n",
            "Epoch: 39, Loss: 0.2279214709997177\n",
            "Epoch: 40, Loss: 0.22253572940826416\n",
            "Epoch: 41, Loss: 0.21728120744228363\n",
            "Epoch: 42, Loss: 0.21215474605560303\n",
            "Epoch: 43, Loss: 0.20715320110321045\n",
            "Epoch: 44, Loss: 0.20227350294589996\n",
            "Epoch: 45, Loss: 0.1975127011537552\n",
            "Epoch: 46, Loss: 0.19286789000034332\n",
            "Epoch: 47, Loss: 0.18833626806735992\n",
            "Epoch: 48, Loss: 0.18391503393650055\n",
            "Epoch: 49, Loss: 0.1796015053987503\n",
            "Epoch: 50, Loss: 0.17539308965206146\n",
            "Epoch: 51, Loss: 0.1712871938943863\n",
            "Epoch: 52, Loss: 0.16728131473064423\n",
            "Epoch: 53, Loss: 0.16337303817272186\n",
            "Epoch: 54, Loss: 0.1595599502325058\n",
            "Epoch: 55, Loss: 0.155839741230011\n",
            "Epoch: 56, Loss: 0.15221016108989716\n",
            "Epoch: 57, Loss: 0.14866895973682404\n",
            "Epoch: 58, Loss: 0.1452140361070633\n",
            "Epoch: 59, Loss: 0.14184324443340302\n",
            "Epoch: 60, Loss: 0.13855458796024323\n",
            "Epoch: 61, Loss: 0.13534601032733917\n",
            "Epoch: 62, Loss: 0.13221554458141327\n",
            "Epoch: 63, Loss: 0.12916134297847748\n",
            "Epoch: 64, Loss: 0.12618151307106018\n",
            "Epoch: 65, Loss: 0.12327424436807632\n",
            "Epoch: 66, Loss: 0.12043776363134384\n",
            "Epoch: 67, Loss: 0.11767036467790604\n",
            "Epoch: 68, Loss: 0.11497034877538681\n",
            "Epoch: 69, Loss: 0.11233606189489365\n",
            "Epoch: 70, Loss: 0.1097659170627594\n",
            "Epoch: 71, Loss: 0.10725834965705872\n",
            "Epoch: 72, Loss: 0.10481181740760803\n",
            "Epoch: 73, Loss: 0.10242485255002975\n",
            "Epoch: 74, Loss: 0.10009598731994629\n",
            "Epoch: 75, Loss: 0.09782382100820541\n",
            "Epoch: 76, Loss: 0.0956069603562355\n",
            "Epoch: 77, Loss: 0.09344404190778732\n",
            "Epoch: 78, Loss: 0.0913337767124176\n",
            "Epoch: 79, Loss: 0.08927487581968307\n",
            "Epoch: 80, Loss: 0.08726606518030167\n",
            "Epoch: 81, Loss: 0.08530614525079727\n",
            "Epoch: 82, Loss: 0.08339390158653259\n",
            "Epoch: 83, Loss: 0.0815282016992569\n",
            "Epoch: 84, Loss: 0.07970787584781647\n",
            "Epoch: 85, Loss: 0.07793186604976654\n",
            "Epoch: 86, Loss: 0.07619905471801758\n",
            "Epoch: 87, Loss: 0.07450840622186661\n",
            "Epoch: 88, Loss: 0.07285886257886887\n",
            "Epoch: 89, Loss: 0.07124944776296616\n",
            "Epoch: 90, Loss: 0.06967919319868088\n",
            "Epoch: 91, Loss: 0.06814710795879364\n",
            "Epoch: 92, Loss: 0.06665229052305222\n",
            "Epoch: 93, Loss: 0.06519383937120438\n",
            "Epoch: 94, Loss: 0.0637708529829979\n",
            "Epoch: 95, Loss: 0.062382444739341736\n",
            "Epoch: 96, Loss: 0.061027806252241135\n",
            "Epoch: 97, Loss: 0.05970609188079834\n",
            "Epoch: 98, Loss: 0.058416496962308884\n",
            "Epoch: 99, Loss: 0.05715826153755188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "raFhhhuGeaIr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}